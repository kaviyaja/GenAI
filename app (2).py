# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pP8A7hbqg0EaS0fswSt3nP_8Aqi0ZAWL
"""

import gradio as gr
import librosa
import numpy as np
import tensorflow as tf
import os
from tensorflow.keras.models import load_model
from sklearn.preprocessing import LabelEncoder
import pickle

# Load Keras model
model = load_model("voice_emotion_model.keras")

# Dummy encoder (create same mapping used in training)
emotions = ['angry', 'happy', 'neutral', 'sad']  # <-- update this based on your actual folders
encoder = LabelEncoder()
encoder.fit(emotions)

# Feature extraction function
def extract_features(file_path):
    try:
        audio, sample_rate = librosa.load(file_path, duration=3, offset=0.5)
        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13)
        return np.mean(mfccs.T, axis=0)
    except Exception as e:
        print(f"Error: {e}")
        return None

# Gradio prediction function
def predict_emotion_gr(audio):
    if audio is None:
        return "No audio uploaded"

    feature = extract_features(audio)
    if feature is not None:
        feature = feature.reshape(1, -1)
        prediction = model.predict(feature)
        predicted_label = encoder.inverse_transform([np.argmax(prediction)])
        return predicted_label[0]
    else:
        return "Could not extract features"

# Create Gradio interface
interface = gr.Interface(
    fn=predict_emotion_gr,
    inputs=gr.Audio(source="upload", type="filepath", label="Upload Voice (.mp3)"),
    outputs=gr.Textbox(label="Predicted Emotion"),
    title="ðŸŽ™ï¸ Voice Emotion Classifier",
    description="Upload a 3-second voice clip (.mp3). The model will predict the emotion."
)

interface.launch()